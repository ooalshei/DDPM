{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUAa8R1VCWe_",
        "outputId": "b344abeb-5781-4ec6-df34-a625d4d604fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MBDataSet(Dataset):\n",
        "\n",
        "    def __init__(self, root): #I think I got this, but ask about other arguments\n",
        "        self.data = np.load(root)\n",
        "        self.max_data = np.max(self.data, axis=0)\n",
        "        self.min_data = np.min(self.data, axis=0)\n",
        "        self.standardized_data = 2*(self.data - self.min_data)/(self.max_data - self.min_data) - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) #need to verify the index here\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.standardized_data[index]\n",
        "        return torch.from_numpy(x).float() #Ask here"
      ],
      "metadata": {
        "id": "DAtivyI2Cboc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        half_dim = self.dim // 2\n",
        "        emb = np.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb"
      ],
      "metadata": {
        "id": "MTMCrAb4CdW3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Reparametrize(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.dim_in = dim_in\n",
        "        self.dim_out = dim_out\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(dim_in, dim_out),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "ycpPRrANCfN8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Reparametrize2(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.dim_in = dim_in\n",
        "        self.dim_out = dim_out\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(dim_in, dim_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "IAqWTkxv2DPy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, data_dim_in, data_dim_out, time_dim_in, time_dim_out):\n",
        "        super().__init__()\n",
        "        self.data_dim_in = data_dim_in\n",
        "        self.data_dim_out = data_dim_out\n",
        "        self.time_dim_in = time_dim_in\n",
        "        self.time_dim_out = time_dim_out\n",
        "\n",
        "\n",
        "        self.data_layer = nn.Sequential(\n",
        "            nn.Linear(data_dim_in, data_dim_out),\n",
        "            nn.Mish()\n",
        "        )\n",
        "\n",
        "        self.time_layer = nn.Sequential(\n",
        "            nn.Linear(time_dim_in, time_dim_out),\n",
        "            nn.Mish()\n",
        "        )\n",
        "\n",
        "        self.combined_data_layer = nn.Sequential(\n",
        "            nn.Linear(data_dim_out, data_dim_out),\n",
        "            nn.Mish()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        h_data = self.data_layer(x)\n",
        "        h_time = self.time_layer(t_emb)\n",
        "        combined_output = h_data + h_time\n",
        "        out = self.combined_data_layer(combined_output)\n",
        "        return out\n",
        "\n",
        "class MLPModule(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_list = [4,8,16,32]):\n",
        "        super().__init__()\n",
        "        self.block_list = nn.ModuleList()\n",
        "#         self.block_list.append(Reparametrize(3,4))\n",
        "        upsample, downsample = dim_list, dim_list[::-1]\n",
        "\n",
        "        for data_dim_in, data_dim_out in zip(upsample[:-1], upsample[1:]):\n",
        "            self.block_list.append(CombinedBlock(data_dim_in, data_dim_out, dim_list[0], data_dim_out))\n",
        "        for data_dim_in, data_dim_out in zip(downsample[:-1], downsample[1:]):\n",
        "            self.block_list.append(CombinedBlock(data_dim_in, data_dim_out, dim_list[0], data_dim_out))\n",
        "#         self.block_list.append(Reparametrize(4,3))\n",
        "\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "\n",
        "        for block in self.block_list:\n",
        "            x = block(x, t_emb)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cKAgFSC3Cikx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data=MBDataSet('/content/data.npy')\n",
        "batch_size=100\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "training_data.__getitem__(0)"
      ],
      "metadata": {
        "id": "2CKj7MUBCtkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d830996-2143-4de2-c7db-c4443a44412b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.7915,  0.4727, -1.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(outputs, targets):\n",
        "    return ((outputs - targets)**2).mean()"
      ],
      "metadata": {
        "id": "7z8H384SCyc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rp_i = Reparametrize(3,4).to('cuda')\n",
        "rp_f = Reparametrize2(4,3).to('cuda')\n",
        "spm = SinusoidalPosEmb(4)\n",
        "mlp = MLPModule().to('cuda')\n",
        "rp_i.train()\n",
        "mlp.train()\n",
        "rp_f.train()\n",
        "params = list(mlp.parameters()) + list(rp_i.parameters()) + list(rp_f.parameters())\n",
        "# loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(params, lr=1e-3)     #preferences?"
      ],
      "metadata": {
        "id": "RjNWQS_lES3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta_start=0.0001\n",
        "beta_end=0.02\n",
        "num_diffusion_steps=1000\n",
        "\n",
        "times = torch.round(torch.linspace(0,1,num_diffusion_steps), decimals=3)\n",
        "betas = torch.linspace(beta_start,beta_end,num_diffusion_steps) # from implementation in Ho, et. al.\n",
        "# we use betas (which are just rescaled times) to parameterize the diffusion process\n",
        "\n",
        "loss_arr = []\n",
        "\n",
        "for epoch in range(15):             #Preferences?\n",
        "\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        time_indices = torch.randint(num_diffusion_steps, (batch_size,1))\n",
        "        sampled_times = times[time_indices]\n",
        "        # sampling a different time for each data sample in batch\n",
        "        t_emb = spm(sampled_times).to('cuda')\n",
        "\n",
        "        alphas=(1-betas).cumprod(dim=0)[time_indices].view(batch_size,1)\n",
        "        # selecting alpha/beta values corresponding to times\n",
        "        noise = torch.normal(0, 1, size=data.shape)\n",
        "        corrupted_data = alphas**0.5*data + (1-alphas)**0.5*noise\n",
        "        noise = noise.to('cuda')\n",
        "        corrupted_data = corrupted_data.to('cuda')\n",
        "        # corrupting data according to timestep using implementation in DDPM, Ho, et. al.\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs = rp_i(corrupted_data)\n",
        "        outputs_prime = mlp(inputs, t_emb)\n",
        "        outputs = rp_f(outputs_prime)\n",
        "\n",
        "        loss = loss_function(outputs, noise)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss_arr.append(loss.detach())\n",
        "        # print(loss_arr())\n",
        "        # if i % 5 == 0:\n",
        "        #     print(loss)\n",
        "\n",
        "        current_loss += loss.item()\n",
        "        if i % 1000 == 999:\n",
        "            print('Loss after mini-batch %5d: %.3f' %\n",
        "                (i + 1, current_loss / 1000))\n",
        "            current_loss = 0.0\n",
        "\n",
        "print('Training is complete')\n"
      ],
      "metadata": {
        "id": "7yDZGXWVC0VV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342d217d-c90c-4600-edce-5fef140d8822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch  1000: 0.776\n",
            "Loss after mini-batch  2000: 0.470\n",
            "Loss after mini-batch  3000: 0.433\n",
            "Loss after mini-batch  4000: 0.429\n",
            "Loss after mini-batch  5000: 0.427\n",
            "Loss after mini-batch  6000: 0.428\n",
            "Loss after mini-batch  7000: 0.424\n",
            "Loss after mini-batch  8000: 0.425\n",
            "Loss after mini-batch  9000: 0.419\n",
            "Loss after mini-batch 10000: 0.419\n",
            "Starting epoch 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rp_i.eval()\n",
        "mlp.eval()\n",
        "rp_f.eval()\n",
        "\n",
        "eta=1\n",
        "\n",
        "sampling_batch_size = 10000\n",
        "\n",
        "x = torch.randn(\n",
        "    sampling_batch_size,\n",
        "    3,\n",
        "    dtype=torch.float\n",
        ")\n",
        "\n",
        "x = x.float()\n",
        "x = x.to('cuda')\n",
        "\n",
        "# betas = torch.from_numpy(\n",
        "#     get_beta_schedule(beta_start, beta_end, num_diffusion_timesteps)).float().to('cuda')\n",
        "\n",
        "seq = range(1, num_diffusion_steps)\n",
        "noise_arr = []\n",
        "std_arr = []\n",
        "with torch.no_grad():\n",
        "    n = x.size(0)\n",
        "    seq_next = [0] + list(seq[:-1])\n",
        "    x0_preds, xs, x_last = [], [x], []\n",
        "    xt_next = x\n",
        "    times = list(zip(reversed(seq), reversed(seq_next)))\n",
        "\n",
        "    for i,j in times:\n",
        "\n",
        "        t = (torch.ones(n) * i)\n",
        "        next_t = (torch.ones(n) * j)\n",
        "        at = (1-betas).cumprod(dim=0)[t.long()].view(sampling_batch_size,1).to('cuda')\n",
        "        at_next = (1-betas).cumprod(dim=0)[next_t.long()].view(sampling_batch_size,1).to('cuda')\n",
        "\n",
        "        xt = xt_next\n",
        "        xt[::,2] = 0\n",
        "\n",
        "        t_emb = spm(t).to('cuda')\n",
        "        xt_prime = rp_i(xt)\n",
        "        et_prime = mlp(xt_prime, t_emb)\n",
        "        et = rp_f(et_prime)\n",
        "\n",
        "        x0_t = (xt - et * (1 - at).sqrt()) / at.sqrt()\n",
        "        x0_t[::,2] = 0\n",
        "\n",
        "        c1 = eta * ((1 - at / at_next) * (1 - at_next) / (1 - at)).sqrt()\n",
        "        c2 = ((1 - at_next) - c1**2).sqrt()\n",
        "        xt_next = at_next.sqrt() * x0_t + c1 * torch.normal(0, 1, size=x.size()).to('cuda') + c2 * et\n",
        "        xt_next[::,2] = 0"
      ],
      "metadata": {
        "id": "01h8y-zzLt5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xt_next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW-B1jBFwye7",
        "outputId": "aa750548-a104-4cea-dd76-09f41538e2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2393,  1.1753,  0.0000],\n",
              "        [-0.2216,  1.1259,  0.0000],\n",
              "        [ 0.1980,  0.6617,  0.0000],\n",
              "        ...,\n",
              "        [ 0.0513,  0.3594,  0.0000],\n",
              "        [-0.7214,  0.8230,  0.0000],\n",
              "        [ 0.0012,  0.5561,  0.0000]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qVNsecQg2zuc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}